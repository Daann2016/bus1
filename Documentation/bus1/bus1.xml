<?xml version='1.0'?> <!--*-nxml-*-->
<!DOCTYPE refentry PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
        "http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">

<refentry id="bus1">

  <refentryinfo>
    <title>bus1</title>
    <productname>bus1</productname>

    <authorgroup>
      <author>
        <contrib>Documentation</contrib>
        <firstname>David</firstname>
        <surname>Herrmann</surname>
      </author>
      <author>
        <contrib>Documentation</contrib>
        <firstname>Tom</firstname>
        <surname>Gundersen</surname>
      </author>
    </authorgroup>
  </refentryinfo>

  <refmeta>
    <refentrytitle>bus1</refentrytitle>
    <manvolnum>7</manvolnum>
  </refmeta>

  <refnamediv>
    <refname>bus1</refname>
    <refpurpose>Kernel Message Bus</refpurpose>
  </refnamediv>

  <refsynopsisdiv>
    <funcsynopsis>
      <funcsynopsisinfo>#include &lt;linux/bus1.h&gt;</funcsynopsisinfo>
    </funcsynopsis>
  </refsynopsisdiv>

  <refsect1> <!-- DESCRIPTION -->
    <title>Description</title>
    <para>
The Bus1 Kernel Message Bus defines and implements a distributed object model.
It allows local processes to send messages to objects owned by remote processes,
as well as share their own objects with others. Object ownership is static and
cannot be transferred. Access to remote objects is prohibited, unless it was
explicitly granted. Processes can transmit messages to a remote object via the
message bus, transferring a data payload, object access rights, file
descriptors, or other auxiliary data.
    </para>
    <para>
To participate on the message bus, a peer context must be created. Peer contexts
are kernel objects, identified by a file descriptor. They are not bound to any
process, but can be shared freely. The peer context provides a message queue to
store all incoming messages, a registry for all locally owned objects, and
tracks access rights to remote objects. A peer context never serves as
routing entity, but merely as anchor for peer-owned resources. Any message on
the bus is always destined at an object, and the bus takes care to transfer a
message into the message queue of the peer context that owns this object.
    </para>
    <para>
The message bus manages object access using capability-based security. That is,
by default only the owner of an object is granted access rights. No other peer
can access the object, nor are they aware of the existance of the object.
However, access rights can be transmitted as auxiliary data with any message,
effectively granting them to the receiver of the message. This even works
transitively, that is, any peer that was granted access to an object can pass on
those rights, even if they do not own the object. But mind that no access rights
can ever be revoked, besides the owner destroying the object.
    </para>

    <refsect2>
      <title>Nodes and Handles</title>
      <para>
Each peer context comes with a registry of owned objects, which in bus1
parlance are called <emphasis>nodes</emphasis>. A peer is always the exclusive
owner of all nodes it has created. Ownership cannot be transferred. Furthermore, 
initially, the node owner is the only peer with access to a newly created node.
The message bus manages access rights to nodes as a set of
<emphasis>handles</emphasis> held by each peer. For each node a peer has access
to, whether it is local or remote, the message bus keeps a handle on the peer.
Those handles are local to each peer, but can be transmitted as auxiliary data
with any message, effectively allocating a new handle to the same node in the
destination peer. This works transitively, and each peer with access rights can
pass them on further, or deliberately drop them again. As long as a peer has
access rights to a node it can send messages to it. However, a node owner can,
at any time, decide to destroy a node. This causes all further message
transactions to this node to fail, and all peers holding access rights to the
node (i.e., they own a handle for that node) are notified of the destruction.
      </para>
      <para>
Handles are the only way to refer to both local and remote nodes. For each
handle allocated on a peer, a 64bit ID is assigned to identify that particular
handle on that particular peer. The ID is only valid locally on that peer. It
cannot be used by remote peers to address the handle (in other words, the ID
namespace is tied to each peer and does not define global entities).
Furthermore, an assigned ID is never reused, even if a handle is dropped. The
kernel keeps a user-reference count for each handle. Every time a handle is
exposed to a peer, the user-reference count of that handle is incremented by
one. This is never done asynchronously, but only synchronously when an ioctl is
called by the holding peer. Therefore, a peer can reliable deduce the current
user-reference count of all its handles, regardless of any ongoing message
transaction. References can be explicitly dropped by a peer. Once the counter of
a handle hits zero, it is destroyed, its ID becomes invalid, and will not be
reused again. Note that a peer can never have multiple different handles to the
same node, rather the kernel always coalesces them into a single handle, using
the user-reference counter to track it. However, if a handle is fully released,
but the peer later acquires a handle to the same node again, its ID will be
different, as IDs are never reused. A concept of soft-references is not
supported.
      </para>
      <para>
When allocating a new node, the node owner implicitly also gets a handle to that
node. As long as the node is valid, the kernel will pin a single user-reference
to the owner's handle. This guarantees that a node owner always retains access
to their node, until they explicitly destroy it (which will also implicitly
release that pinned user-reference on the handle). Otherwise, a handle to a
local node behaves just like any other handle, that is, user-references are
acquired and released according to its use. However, whenever the overall sum of
all user-references on all handles to a node drops to 1 (which implies that only
the pinned reference of the owner is left), a notification is queued on the node
owner. If the counter is incremented again, any such notification is dropped, if
not already dequeued.
      </para>
    </refsect2>

    <refsect2>
      <title>Message Transactions</title>
      <para>
<!--
  XXX: * Unicasts, multicasts
       * memory data, auxiliary data, metadata
       * quotas
       * single-copy, pool-slice
  -->
To be defined.
      </para>
    </refsect2>

    <refsect2>
      <title>Global Ordering</title>
      <para>
<!--
  XXX: Causal ordering, side-channels, unicast/multicast
  -->
To be defined.
      </para>
    </refsect2>

    <refsect2>
      <title>Operating on a bus1 file descriptor</title>
      <para>
The bus1 peer file descriptor supports the following operations:
      </para>
      <variablelist>
        <varlistentry> <!-- FOPS OPEN -->
          <term>
            <citerefentry>
              <refentrytitle>open</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <listitem>
            <para>
A call to
<citerefentry>
  <refentrytitle>open</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>
on the bus1 character device (usually <filename>/dev/bus1</filename>) creates a
new peer context identifie by the returned file descriptor.
            </para>
          </listitem>
        </varlistentry> <!-- FOPS OPEN -->

        <varlistentry> <!-- FOPS POLL -->
          <term>
            <citerefentry>
              <refentrytitle>poll</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <term>
            <citerefentry>
              <refentrytitle>select</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <term>(and similar)</term>
          <listitem>
            <para>
The file descriptor supports
<citerefentry>
  <refentrytitle>poll</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>
(and analogously
<citerefentry>
  <refentrytitle>epoll</refentrytitle><manvolnum>7</manvolnum>
</citerefentry>) and
<citerefentry>
  <refentrytitle>select</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>, as follows:
            </para>

            <itemizedlist>
              <listitem>
                <para>
The file descriptor is readable (the <varname>readfds</varname> argument of
<citerefentry>
  <refentrytitle>select</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>;
the <constant>POLLIN</constant> flag of
<citerefentry>
  <refentrytitle>poll</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>)
if one or more messages are ready to be dequeued.
                </para>
              </listitem>

              <listitem>
                <para>
The file descriptor is writable (the <varname>writefds</varname> argument of
<citerefentry>
  <refentrytitle>select</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>;
the <constant>POLLOUT</constant> flag of
<citerefentry>
  <refentrytitle>poll</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>)
if the peer has not been shut down, yet (i.e., the peer can be used to send
messages).
                </para>
              </listitem>

              <listitem>
                <para>
The file descriptor signals a hang-up (overloaded on the
<varname>readfds</varname> argument of
<citerefentry>
  <refentrytitle>select</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>;
the <constant>POLLHUP</constant> flag of
<citerefentry>
  <refentrytitle>poll</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>)
if the peer has been shut down.
                </para>
              </listitem>
            </itemizedlist>

            <para>
The bus1 peer file descriptor also supports the other file descriptor
multiplexing APIs:
<citerefentry>
  <refentrytitle>pselect</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>, and
<citerefentry>
  <refentrytitle>ppoll</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>.
            </para>
          </listitem>
        </varlistentry> <!-- FOPS POLL -->

        <varlistentry> <!-- FOPS MMAP -->
          <term>
            <citerefentry>
              <refentrytitle>mmap</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <listitem>
            <para>
A call to
<citerefentry>
  <refentrytitle>mmap</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>
installs a memory mapping to the message pool of the peer into the caller's
address-space. No writable mappings are allowed. Furthermore, the pool has no
fixed size, but grows dynamically with the demands of the peer.
            </para>
          </listitem>
        </varlistentry> <!-- FOPS MMAP -->

        <varlistentry> <!-- FOPS IOCTL -->
          <term>
            <citerefentry>
              <refentrytitle>ioctl</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <listitem>
            <para>
The following bus1-specific commands are supported:
            </para>
            <variablelist>
              <varlistentry>
                <term><constant>BUS1_CMD_PEER_RESET</constant></term>
                <listitem>
                  <para>
This command resets a peer context to its initial state. It takes a 64-bit flags
argument. If no flags are passed, all nodes owned by this peer context are
destroyed atomically, followed by a release of all held handles. Lastly, all
remaining slices are flushed from the pool. Only the node destruction is atomic.
The remaining cleanup is not. This means, if another command is executed in
parallel, it might be partially affected by the ongoing reset.
                  </para>
                  <para>
Any node marked as persistent, as well as the seed message, are preserved and
will survive a reset operation.
                  </para>
                  <para>
If the <constant>BUS1_RESET_FLAG_DISCONNECT</constant> is set, no new
operations can be performed on the peer context and all resources, including
persistent nodes, are released.
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_HANDLE_TRANSFER</constant></term>
                <listitem>
                  <para>
This command transfers a handle from one peer context to another. It takes the
following structure as argument:
<programlisting>
struct bus1_cmd_handle_transfer {
        __u64 flags;
        __u64 src_handle;
        __u64 dst_fd;
        __u64 dst_handle;
};
</programlisting>
<varname>flags</varname> must always be set to 0, <varname>src_handle</varname>
is the handle ID of the handle being transferred in the source context,
<varname>dst_fd</varname> is the file descriptor representing the destination
peer context and <varname>dst_handle</varname> must be
<constant>BUS1_HANDLE_INVALID</constant> and is set to the new handle ID in the
destination context on return.
		</para>
		<para>
If the <constant>BUS1_NODE_FLAG_ALLOCATE</constant> flag is set in
<varname>src_handle</varname> a new node is allocated in the source peer context
and <varname>src_handle</varname> is set to its handle ID on return. If
<varname>dst_fd</varname> is set to <constant>-1</constant> the source context
is also used as the destination.
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_HANDLE_RELEASE</constant></term>
                <listitem>
                  <para>
This command releases one user reference to a handle. It takes a handle ID as
argument.
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_NODE_DESTROY</constant></term>
                <listitem>
                  <para>
This command destroys a node. It takes a handle ID as argument. The referenced
handle must be the owner handle of a local node.
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_SLICE_RELEASE</constant></term>
                <listitem>
                  <para>
This command releases one slice from the local pool. It takes a pool offset to
the start of the slice to be released.
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_SEND</constant></term>
                <listitem>
                  <para>
This command sends a message. It takes the following structure as argument:
<programlisting>
struct bus1_cmd_send {
        __u64 flags;
        __u64 ptr_destinations;
        __u64 n_destinations;
        __u64 ptr_vecs;
        __u64 n_vecs;
        __u64 ptr_handles;
        __u64 n_handles;
        __u64 ptr_fds;
        __u64 n_fds;
};
</programlisting>
                  </para>
                  <para>
<varname>flags</varname> may be set to at most one of
<constant>BUS1_SEND_FLAG_CONTINUE</constant> and
<constant>BUS1_SEND_FLAG_SEED</constant>. If
<constant>BUS1_SEND_FLAG_CONTINUE</constant> is set any messages that cannot
be delivered due to errors on the remote peer are silently dropped and the
error is instead reported to the receiver, otherwise such an error means the
whole transaction fails. If <constant>BUS1_SEND_FLAG_SEED</constant> is set the
message replaces the seed message on the local peer. In this case,
<varname>n_destinations</varname> must be 0.
                  </para>
                  <para>
<varname>ptr_destinations</varname> is a pointer to an array of handle IDs and
<varname>n_destinations</varname> is the length of the array. If the
<constant>BUS1_NODE_FLAG_ALLOCATE</constant> is set in either of the handle IDs
a new node is allocated in the local peer context, and the handle ID is replaced
by the ID of the new node on return. The message being sent is delivered to
the peer context owning the nodes pointed to by each of the handles in the
array.
                  </para>
                  <para>
<varname>ptr_vecs</varname> is a pointer to an array of iovecs and
<varname>n_vecs</varname> is the length of the array. The iovecs represent the
payload of the message which is delivered to each destination.
                  </para>
                  <para>
<varname>ptr_handles</varname> is a pointer to an array of handle IDs and
<varname>n_handles</varname> is the length of the array. Nodes may be allocated
on-demand as described above. Each of the handles in this array is installed
in each destination peer context at receive time.
                  </para>
                  <para>
<varname>ptr_fds</varname> is a pointer to an integer array of file descriptors
and <varname>n_fds</varname> is the length of the array. Each of the file
descriptors in this array may be installed in the destination peer context at
receive time (see below).
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term><constant>BUS1_CMD_RECV</constant></term>
                <listitem>
                  <para>
This command receives a message. It takes the following structure as argument:
<programlisting>
struct bus1_cmd_recv {
        __u64 flags;
        __u64 n_dropped;
        struct {
                __u64 type;
                __u64 destination;
                __u32 uid;
                __u32 gid;
                __u32 pid;
                __u32 tid;
                __u64 offset;
                __u64 n_bytes;
                __u64 n_handles;
                __u64 n_fds;
        } msg;
};
</programlisting>
If <constant>BUS1_RECV_FLAG_PEEK</constant> is set in <varname>flags</varname>,
the received message is not dropped from the queue. If
<constant>BUS1_RECV_FLAG_SEED</constant> is set, the peer's seed is received
rather than a message from the queue. If
<constant>BUS1_RECV_FLAG_INSTALL_FDS</constant> the file descriptors attached to
the received message are installed in the receiving process. Care must be taken
when using the last flag from more than one process on the same message as file
descriptor numbers are per-process and not per-peer.
                  </para>
                  <para>
<varname>n_dropped</varname> indicates the number of silently dropped messages
destined for this peer. The count is reset unless the
<constant>BUS1_RECV_FLAG_PEEK</constant> flag was passed.
                  </para>
                  <para>
<varname>msg.type</varname> indicates the type of message.
<constant>BUS1_MSG_NONE</constant> is used to only receive
<varname>n_dropped</varname>. <constant>BUS1_MSG_DATA</constant> indicates a
regular message sent from another peer, possibly containing a payload, as well
as attached handles and filedescriptors.
<constant>BUS1_MSG_NODE_DESTROY</constant> indicates that the node referenced
by the handle in <varname>msg.destination</varname> was destroyed by its owner.
<constant>BUS1_MSG_NODE_RELEASE</constant> indicates that all the references to
handles referencing the node in <varname>msg.destination</varname> have been
released.
                  </para>
                  <para>
<varname>msg.destination</varname> is the ID of the destination node or handle
of the message.
                  </para>
                  <para>
<varname>msg.uid</varname>, <varname>msg.gid</varname>,
<varname>msg.pid</varname>, and <varname>msg.tid</varname> are the user, group,
process and thread ID of the process that created the sending peer context.
                  </para>
                  <para>
<varname>msg.offset</varname> is the offset, in bytes, into the pool of the
payload and <varname>msg.n_bytes</varname> is its length.
                  </para>
                  <para>
<varname>msg.n_handles</varname> is the number of handles attached to the
message. The handle IDs are stored in the pool following the payload (and
possibly padding to make the array 8-byte aligned).
                  </para>
                  <para>
<varname>msg.n_fds</varname> is the number of handles attached to the
message, or 0 if <constant>BUS1_RECV_FLAG_INSTALL_FDS</constant> was not set.
The file descriptor numbers are stored in the pool following the handle array
(and possibly padding to make the array 8-byte aligned).
                  </para>
                </listitem>
              </varlistentry>
            </variablelist>
          </listitem>
        </varlistentry> <!-- FOPS IOCTL -->

        <varlistentry> <!-- FOPS CLOSE -->
          <term>
            <citerefentry>
              <refentrytitle>close</refentrytitle>
              <manvolnum>2</manvolnum>
            </citerefentry>
          </term>
          <listitem>
            <para>
A call to
<citerefentry>
  <refentrytitle>close</refentrytitle><manvolnum>2</manvolnum>
</citerefentry>
releases the passed file descriptor. When all file descriptors associated with
the same peer context have been closed, the peer is shut down. This destroys all
nodes of that peer, releases all handles, flushes its queue and pool, and
deallocates all related resources. Messages that have been sent by the peer and
are still queued on destination queues, are unaffected by this.
            </para>
          </listitem>
        </varlistentry> <!-- FOPS CLOSE -->
      </variablelist>
    </refsect2>
  </refsect1> <!-- DESCRIPTION -->

  <refsect1> <!-- RETURN VALUE -->
    <title>Return value</title>
    <para>
All bus1 operations return zero on success. On failure, a negative error code is
returned.
    </para>
  </refsect1> <!-- RETURN VALUE -->

  <refsect1> <!-- ERRORS -->
    <title>Errors</title>
    <para>
These are all standard errors generated by the bus layer. See the description
of each ioctl for details on their occurrence.
    </para>
    <variablelist>
      <varlistentry>
        <term><constant>EAGAIN</constant></term>
        <listitem><para>
No messages ready to be read.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EBADF</constant></term>
        <listitem><para>
Invalid file descriptor.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EDQUOT</constant></term>
        <listitem><para>
Resource quota exceeded.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EFAULT</constant></term>
        <listitem><para>
Cannot read, or write, ioctl parameters.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EHOSTUNREACH</constant></term>
        <listitem><para>
The destination object is no longer available.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EINVAL</constant></term>
        <listitem><para>
Invalid ioctl parameters.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EMSGSIZE</constant></term>
        <listitem><para>
The message to be sent exceeds its allowed resource limits.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>ENOMEM</constant></term>
        <listitem><para>
Out of kernel memory.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>ENOTTY</constant></term>
        <listitem><para>
Unknown ioctl.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>ENXIO</constant></term>
        <listitem><para>
Unknown object.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EOPNOTSUPP</constant></term>
        <listitem><para>
Operation not supported.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>EPERM</constant></term>
        <listitem><para>
Permission denied.
        </para></listitem>
      </varlistentry>

      <varlistentry>
        <term><constant>ESHUTDOWN</constant></term>
        <listitem><para>
Local peer was already shut down.
        </para></listitem>
      </varlistentry>
    </variablelist>
  </refsect1> <!-- ERRORS -->

  <refsect1> <!-- SEE ALSO -->
    <title>See Also</title>
    <simplelist type="inline">
      <member>
        <citerefentry>
          <refentrytitle>bus1.peer</refentrytitle>
          <manvolnum>7</manvolnum>
        </citerefentry>
      </member>
      <member>
        <citerefentry>
          <refentrytitle>bus1.node</refentrytitle>
          <manvolnum>7</manvolnum>
        </citerefentry>
      </member>
      <member>
        <citerefentry>
          <refentrytitle>bus1.message</refentrytitle>
          <manvolnum>7</manvolnum>
        </citerefentry>
      </member>
      <member>
        <citerefentry>
          <refentrytitle>bus1.pool</refentrytitle>
          <manvolnum>7</manvolnum>
        </citerefentry>
      </member>
    </simplelist>
  </refsect1> <!-- SEE ALSO -->

</refentry>
